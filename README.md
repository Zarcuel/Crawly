Crawly
Crawly is a fast, parallel processing web scanner that crawls websites to detect privacy-impacting elements like tracking cookies, social media embeds, and analytics scripts using Playwright and BeautifulSoup. It can also run in "scraper mode" to search for specific keywords or file types (like *.pdf or login) across a site.

ğŸ”§ Features
ğŸ” Detects tracking cookies, social media plugins, analytics tools, and suspicious domains

ğŸ•µï¸ Scraper mode: search for patterns like *.pdf, login, or any keyword in the page content

âš¡ Fast, multithreaded scanning with ThreadPoolExecutor

ğŸ­ Uses Playwright to simulate real browser visits and collect JavaScript-set cookies

ğŸ§  Smart filtering of URLs and crawl depth

ğŸ’¾ Exports to CSV and optionally JSON

â¹ï¸ Graceful stopping with q + Enter

ğŸ“¤ Re-export CSV from saved JSON (no need to rescan)

ğŸ’» Installation
Requirements
Python 3.7+

Playwright

Install Crawly (for development)
bash
Copy
Edit
pip install beautifulsoup4 playwright
playwright install
ğŸš€ Usage
bash
Copy
Edit
python crawly.py -u <start_url> [options]
Or if installed via pip:

bash
Copy
Edit
crawly-zarcuel -u <start_url> [options]
Options
Option	Description
-u, --url	Required. Starting URL to scan
-d	Crawl depth (default: 2)
-e, --exclude	Substrings to exclude from crawling (e.g., logout, contact)
-w, --workers	Number of parallel threads (default: 4)
-o, --output	Output CSV filename (default: scan_results.csv)
-s, --scan-limit	Maximum number of pages to scan
-f, --filter	Substrings to filter from final CSV output
--scrape	Scraper mode: keyword or wildcard pattern (e.g., "*.pdf", "token")
--save-json	Save raw scan results to a .json file
--reexport	Re-export CSV from saved JSON file (no new scan required)

ğŸ” Scraper Mode
Instead of checking for trackers, use Crawly as a simple scraper:

bash
Copy
Edit
python crawly.py -u https://example.com --scrape "login"
python crawly.py -u https://example.com --scrape "*.pdf"
It will return only pages containing the specified string or pattern.

âœ‹ Graceful Exit
Press q + Enter during a scan to stop crawling early and immediately begin scanning the collected URLs.

ğŸ§¾ Output Format
The CSV output contains:

URL: scanned page

Detected: Yes/No for plugin/cookie mode

Source: matched sources (scripts, embeds, etc.)

Cookies: matched cookies and headers

ğŸ” Reuse Past Results
You can re-filter or re-export a past scan without re-scanning:

bash
Copy
Edit
python crawly.py --reexport saved_results.json -o new_filtered_report.csv -f cdn.example.com
ğŸ‘¨â€ğŸ’» Author
Zarcuel â€” Privacy-focused pentester and creator of Crawly ğŸ•·ï¸
MIT Licensed â€“ Free for personal and commercial use

Let me know if youâ€™d like badges (like PyPI, license, or version), screenshots of output, or a sample CSV snippet added!
